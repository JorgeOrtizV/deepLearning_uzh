{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number\n",
    "- Name, matriculation number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Classification in PyTorch \n",
    "\n",
    "\n",
    "For this exercise, we will switch to an implementation in PyTorch. \n",
    "The goal of this exercise is to get used to some concepts in PyTorch, such as relying on the `torch.tensor` data structure, implementing the network, the loss functions, the training loop and accuracy computation, which we will apply to binary and categorical classification.\n",
    "\n",
    "Please make sure that all your variables are compatible with `torch`.\n",
    "For example, you cannot mix `torch.tensor`s and `numpy.ndarray`s in any part of the code.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use two different datasets, the *churn* dataset https://archive.ics.uci.edu/dataset/563/iranian+churn+dataset for binary classification and the *winequality-red* dataset https://archive.ics.uci.edu/dataset/186/wine+quality for categorical classification. Both datasets are avaliable on the UCI Machine Learning repository.\n",
    "\n",
    "The binary classification dataset contains features extracted from customers of a telecommunication company, which are classified as either churn or not. \n",
    "The categorical classification dataset contains chemical measurements for six distinct qualities of a Portuguese white wine.\n",
    "In the former dataset, the class is indicated in the final column named \"Churn\", whereas for the latter, target information is provided in the last column named \"quality.\"\n",
    "\n",
    "Please run the code block below to download the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# download the two dataset files\n",
    "dataset_files = {\n",
    "  \"churn_data.zip\": \"https://archive.ics.uci.edu/static/public/563/iranian+churn+dataset.zip\",\n",
    "  \"winequality-red.csv\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "}\n",
    "for name, url in dataset_files.items():\n",
    "    # Skip downloading if the file already exists\n",
    "    if not os.path.exists(name):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, name)\n",
    "        print(f\"Downloaded {name} successfully.\")\n",
    "\n",
    "        # get the extention of file\n",
    "        base,extension = os.path.splitext(name)\n",
    "        if extension == \".zip\":\n",
    "            # Extract the zip file\n",
    "            with zipfile.ZipFile(name, 'r') as zip_ref:\n",
    "                zip_ref.extractall()\n",
    "            print(f\"Extracted {name} successfully.\")\n",
    "            # rename the file\n",
    "            os.rename(\"Customer Churn.csv\", base + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Dataset Loading\n",
    "\n",
    "The first task deals with the loading of the datasets. \n",
    "When training networks in PyTorch, all data needs to be stored as datatype ``torch.tensor``. \n",
    "The data should be split between input sets $\\mathbf X = [\\vec x^{[1]}, \\ldots, \\vec x^{[N]}]^T \\in \\mathbb R^{N\\times D}$ and targets.\n",
    "There is **no need to add a bias neuron to the input**, and the transposition of the data matrix is different from what we have seen before.\n",
    "\n",
    "For the targets, we have to be more careful as there are differences w.r.t. the applied loss function.\n",
    "For binary classification, we need $\\mathbf T = [[t^{[1]}, \\ldots, t^{[N]}]]$ to be in dimension $\\mathbb R^{N\\times1}$ and of type ``torch.float``.\n",
    "For categorical classification, we only need the class indexes $\\vec t = [t^{[1]}, \\ldots, t^{[N]}]$ to be in dimension $\\mathbb N^N$ and of type ``torch.long``.\n",
    "\n",
    "Implement a function that returns both the input and the target data for a given dataset\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You can use `csv.reader()` to read the dataset, or rely on other methods such as `pandas`.\n",
    "2. Please note that in the wine dataset CSV file, all values are separated by `;`, whereas in the churn dataset, they are separated by `,`.\n",
    "3. For the wine dataset, subtract the target by `-3` to get the target values in range $\\{0, 1, 2, 3, 4, 5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def dataset(dataset_file=\"winequality-red.csv\",delimiter=\";\"):\n",
    "  # read dataset\n",
    "  dataset = pd.read_csv(dataset_file, sep=delimiter)\n",
    "\n",
    "  print (f\"Loaded dataset with {len(dataset)} samples\")\n",
    "  \n",
    "  # convert to torch.tensor\n",
    "  data = torch.from_numpy(dataset.values)\n",
    "\n",
    "  # get the input (data samples) without the target information\n",
    "  X = data[:, :-1].type(torch.float32)\n",
    "  \n",
    "  if dataset_file == \"winequality-red.csv\":\n",
    "    # target is in the last column and needs to be converted to long\n",
    "    T = data[:,-1].to(torch.long)\n",
    "    # Substract 3 to have target variable from 0 to 5\n",
    "    T -= 3\n",
    "  else:\n",
    "    # target is in the last column and needs to be of type float\n",
    "    T = data[:,-1:].type(torch.float32)\n",
    "\n",
    "  # Reshape target to satisfy Nx1 dimensionality\n",
    "  return X, T.view(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Dataset Check\n",
    "\n",
    "Test 1 assures the correctness of the data and target dimensions.\n",
    "\n",
    "1. For the churn data, we assure that all dimensions are correct and that class labels are in range $\\{0, 1\\}$.\n",
    "\n",
    "2. For the wine dataset, we make sure that the dataset is in the correct dimensions, i.e., $\\mathbf X\\in \\mathbb R^{N\\times D}$ and $\\mathbf T \\in \\mathbb N^N$. And all class labels are in the correct range $[0, O-1]$ where $O$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3150 samples\n",
      "Loaded dataset with 1599 samples\n"
     ]
    }
   ],
   "source": [
    "X, T = dataset(\"churn_data.csv\",\",\")\n",
    "\n",
    "assert X.shape[1] == 13, X.shape[1]\n",
    "assert T.shape[1] == 1, T.shape[1]\n",
    "assert torch.all(T >= 0) and torch.all(T <= 1)\n",
    "assert T.dtype == torch.float\n",
    "\n",
    "X, T = dataset(\"winequality-red.csv\",\";\")\n",
    "\n",
    "assert X.shape[1] == 11, X.shape[1]\n",
    "assert torch.all(T >= 0) and torch.all(T <= 5)\n",
    "assert T.dtype == torch.long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split Training and Validation Data\n",
    "\n",
    "The data should be split into 80% for training and 20% for validation. Implement a function that takes the full dataset $(X,T)$ and returns $(X_t, T_t, X_v, T_v)$ accordingly.\n",
    "\n",
    "Write a function that splits off training and validation samples from a given dataset. **What do we need to assure before splitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_training_data(X,T,train_percentage=0.8):\n",
    "  \n",
    "  # split into 80/20 training/validation\n",
    "  dataset_size = X.shape[0]\n",
    "  size_train = int(dataset_size*train_percentage)\n",
    "  # Create random permutations of 0 to n-1\n",
    "  idxs = np.random.permutation(dataset_size)\n",
    "  train_idxs = idxs[:size_train]\n",
    "  val_idxs = idxs[size_train:]\n",
    "  X_train = X[train_idxs, :]\n",
    "  T_train = T[train_idxs, :]\n",
    "  X_val = X[val_idxs, :]\n",
    "  T_val = T[val_idxs, :]\n",
    "\n",
    "  return X_train, T_train, X_val, T_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Input Data Standardization\n",
    "\n",
    "As we have seen last week, the standardization of the data provides many advantages. \n",
    "Hence, in this task you should write a function that takes $(X_t,X_v)$ as input and standardizes them by subtracting the mean and dividing by the \n",
    "standard deviation of $X_t$, and returning the standardized versions of both. Assure that each input dimension is standardized individually.\n",
    "\n",
    "Implement a function that standardizes all input data for the training and validation set.\n",
    "Return the standardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Use `torch.mean()` and `torch.std()` with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_val):\n",
    "  # compute statistics\n",
    "  # We do this considering the training dataset\n",
    "  mean = torch.mean(X_train, dim=0) # We want it per col\n",
    "  std = torch.std(X_train, dim=0)\n",
    "\n",
    "  # standardize both X_train and X_val\n",
    "  X_train = (X_train - mean)/std\n",
    "  X_val = (X_val - mean)/std\n",
    "  return X_train, X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Implementation\n",
    "\n",
    "We will use a two-layer fully-connected network with $D$ input neurons, $K$ hidden neurons and $O$ output neurons. \n",
    "Depending on the task, $D$ and $O$ need to be selected appropriately, while $K$ is a parameter to play around with. \n",
    "In PyTorch, the easiest way to implement a network is by providing the requested sequence of layers to `torch.nn.Sequential`, which will build a network containing the given layers. \n",
    "We will use two `torch.nn.Linear` layers and one `torch.nn.Tanh` activation function in between. \n",
    "The network will return the logits $\\vec z$ for a given input $\\vec x$.\n",
    "\n",
    "\n",
    "### Task 4: Implement Network\n",
    "\n",
    "Implement a two-layer fully-connected network in PyTorch. \n",
    "The given network uses $\\tanh$ as activation function, and provide the possibility to change the number of inputs $D$, the number of hidden neurons $K$ and the number of outputs $O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def Network(D, K, O):\n",
    "  return torch.nn.Sequential(\n",
    "      # First fully connected layer\n",
    "      torch.nn.Linear(D,K),\n",
    "      # Activation function\n",
    "      torch.nn.Tanh(),\n",
    "      # Second fully connected layer\n",
    "      torch.nn.Linear(K, O)\n",
    "      # Softmax activation\n",
    "      # If we are using the logits then we don't use softmax\n",
    "      # nn.Softmax()\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Accuracy Computation\n",
    "\n",
    "To monitor the training process, we want to compute the accuracy. \n",
    "The function will obtain the logits $\\vec z$ extracted from the network and the according target $t$. \n",
    "Assure that this function works both for binary and categorical classification. \n",
    "\n",
    "Note: you can make use of the following pytorch functions:\n",
    "\n",
    "1. `torch.mean()` which computes the mean or average of the input tensor.\n",
    "2. `torch.argmax()` which returns the indices of the maximum values of all elements of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Z, T):\n",
    "  # check if we have binary or categorical classification\n",
    "  if Z.shape[1] == 1: # We have only one target column\n",
    "    # binary classification\n",
    "    preds = torch.round(torch.sigmoid(Z))\n",
    "    return torch.sum(preds==T).item()/Z.shape[0]\n",
    "  else:\n",
    "    # categorical classification\n",
    "    _, preds = torch.max(torch.nn.functional.softmax(Z, dim=1),dim=1)\n",
    "    return torch.sum(preds==T).item() / Z.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Test Accuracy Function\n",
    "\n",
    "Test 2 assures the correctness of your accuracy function in both binary and categorical cases. We make sure that the accuracy will compute the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, test binary classification\n",
    "ZZ = torch.ones((20,1)) * -5.\n",
    "ZZ[15:20] = 5\n",
    "assert(abs(accuracy(ZZ,torch.zeros((20,1))) - 0.75) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones((20,1))) - 0.25) < 1e-8)\n",
    "\n",
    "# now, test categorical classification with 4 classes\n",
    "ZZ = torch.ones((20,4)) * -5\n",
    "ZZ[0:1,0] = 5\n",
    "ZZ[1:4,1] = 5\n",
    "ZZ[4:10,2] = 5\n",
    "ZZ[10:20,3] = 5\n",
    "\n",
    "assert(abs(accuracy(ZZ,torch.zeros(20)) - 0.05) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)) - 0.15) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*2) - 0.3) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*3) - 0.5) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.tensor((0,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3))) - 1.) < 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Training Loop\n",
    "\n",
    "Implement a function that takes all necessary parameters to run a training on a given dataset.\n",
    "In this week, we will run gradient descent, i.e., we will train on the whole dataset in each training step, so there is no need to define anything related to batches. \n",
    "Select the optimizer to be `torch.optim.SGD`. \n",
    "\n",
    "Implement a training loop over **10'000** epochs with a learning rate of **$\\eta=0.1$**. \n",
    "Make sure that you train on the training data only, and **not** on the validation data.\n",
    "In each loop, compute and store the training loss, training accuracy, validation loss and validation accuracy. \n",
    "At the end, return the lists of these values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. When storing accuracy or loss values in a list, make sure to convert the to float via `v.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(X_train, T_train, X_val, T_val, model, loss_fn, epochs=10000, lr=0.01, momentum=0):\n",
    "  opt = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "  # collect loss and accuracy values\n",
    "  train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    # Zeroing the grad\n",
    "    opt.zero_grad()\n",
    "    # train on training set\n",
    "    # ... compute network output on training data\n",
    "    predictions = model(X_train)\n",
    "    # ... compute loss from network output and target data\n",
    "    loss = loss_fn(predictions, T_train)\n",
    "    # ... perform parameter update\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # ... remember loss\n",
    "    train_loss.append(loss.item())\n",
    "    # ... compute training set accuracy\n",
    "    train_acc.append(accuracy(predictions, T_train))\n",
    "\n",
    "    # test on validation data\n",
    "    with torch.no_grad():\n",
    "      # ... compute network output on validation data\n",
    "      preds = model(X_val)\n",
    "      # ... compute loss from network output and target data\n",
    "      loss_val = loss_fn(preds,T_val)\n",
    "      # ... remember loss\n",
    "      val_loss.append(loss_val.item())\n",
    "      # ... compute validation set accuracy\n",
    "      val_acc.append(accuracy(preds,T_val))\n",
    "\n",
    "  # return the four lists of losses and accuracies\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Finally, we want to train our network on our data and plot the accuracy and loss values that were obtained through the epochs. \n",
    "Exemplary plots can be found in the exercise slides.\n",
    "\n",
    "\n",
    "### Task 7: Plotting Function\n",
    "\n",
    "Implement a function that takes four lists containing the training loss, the training accuracy, the validation loss and the validation accuracy and plot them into two plots. \n",
    "The first plot should contain the loss values for both training and validation. The second plot should contain the according accuracy values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You might need to convert remaining `torch.tensor` values to `float`, lists, or `numpy.nadrray` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(train_loss, train_acc, val_loss, val_acc):\n",
    "  pyplot.figure(figsize=(10,3))\n",
    "  ax = pyplot.subplot(121)\n",
    "  ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "  ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(122)\n",
    "  ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Binary Classification\n",
    "\n",
    "\n",
    "1. Load the data for binary classification, using the ``\"churn_data.csv\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Standardize both training and validation input data using the function from Task 3.\n",
    "4. Instantiate a network with the correct number of input neurons, a reasonable number of $K$ hidden neurons and one output neuron.\n",
    "\n",
    "**Which loss function do we need for this task?**\n",
    "\n",
    "Train the network on the churn data with the learning rate of **$\\eta=0.1$** for 10'000 epochs and plot the training and validation accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 3150 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:10<00:00, 952.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "# load dataset\n",
    "X, T = dataset('churn_data.csv', ',')\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X,T)\n",
    "# standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "# initiate the network\n",
    "network = Network(D=X_train.shape[1], K=5, O=1)\n",
    "\n",
    "#print(X_train.dtype)\n",
    "# train network on our data\n",
    "results = train(X_train, T_train, X_val, T_val, network, loss)\n",
    "#train_loss, train_acc, val_loss, val_acc = results\n",
    "# plot the results\n",
    "#plot(train_loss, train_acc, val_loss, val_acc)\n",
    "#plot(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9007936507936508 0.9174603174603174\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, val_loss, val_acc = results\n",
    "# For some reason my kernel dies when attempting to plot\n",
    "#plot(train_loss, train_acc, val_loss, val_acc)\n",
    "print(max(train_acc), max(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering accuracies exceeding 90%, do you think our evaluation might be flawed? If so, what aspects could we be neglecting?\n",
    "\n",
    "We might have a class imbalance or overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Categorical Classification\n",
    "\n",
    "1. Load the data for categorical classification, using the ``\"winequality-red.csv\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Standardize both training and validation input data using the function from Task 3.\n",
    "4. **How many input and output neurons do we need?** Change the number of input, hidden, and output neurons accordingly.\n",
    "4. Build a network with these params you decide.\n",
    "\n",
    "**Which loss function do we need this time?**\n",
    "\n",
    "Train the network on the wine data with the learning rate of **$\\eta=0.1$** for 10'000 epochs and plot the training and validation accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1599 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:12<00:00, 827.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# load dataset\n",
    "X, T = dataset('winequality-red.csv', ';')\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X,T)\n",
    "# standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "# initiate the network\n",
    "network = Network(D=X_train.shape[1], K=5, O=torch.max(T).item()+1) #+1 because we have from 0 to 5, therefore 6 items\n",
    "\n",
    "#print(X_train.dtype)\n",
    "# train network on our data\n",
    "#print(T_train.flatten())\n",
    "results = train(X_train, T_train.flatten(), X_val, T_val.flatten(), network, loss)\n",
    "train_loss, train_acc, val_loss, val_acc = results\n",
    "# plot the results\n",
    "plot(train_loss, train_acc, val_loss, val_acc)\n",
    "#plot(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reviewing the plots, do you identify any potential concerns? If so, what actions could be taken to mitigate?\n",
    "\n",
    "Some concerns are that the validation loss is increasing, which implies overfitting. This can be mitigated by regularization, reducing model complexity, early stopping, or getting more training data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
